{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - Notes\n",
    "\n",
    "---\n",
    "\n",
    "## Logistic Regression Model\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "The traditional cost function from the linear regression model cannot be used in the logistic regression model, because when combined with the Sigmoid function, the cost J(&theta;) would appear \"non-convex\", meaning it would be full of local minima. This would render gradient descent largely ineffective, as we could not guarantee that it settles at the parameters &theta; that ensure a global minimum for J(&theta;).\n",
    "\n",
    "J(&theta;) = 1/m * SUM(Cost(h(x), y))\n",
    "\n",
    "**Traditional cost function:** \n",
    "\n",
    "Cost(h(x), y) = 1/2(h(x) - y))<sup>2</sup>\n",
    "\n",
    "**Logistic regression cost function:** \n",
    "\n",
    "_If y = 1:_ Cost(h(x), y) = -log(h(x))\n",
    "\n",
    "_If y = 0:_ Cost(h(x), y) = -log(1 - h(x))\n",
    "\n",
    "From these equations, if y = 1 and h(x) = 1, Cost = 0. If y = 1 and h(x) = 0, Cost = infinity. The opposites are true when y = 0.\n",
    "\n",
    "**Simplified cost function:** Cost(h(x), y) = -ylog(h(x)) - (1 - y)log(1 - h(x))\n",
    "\n",
    "Then, to fit the parameters &theta;, we minimize J(&theta;) with respect to &theta;. We use gradient descent to minimize the cost J(&theta;).\n",
    "\n",
    "![Gradient descent](https://i.imgur.com/kxCHver.png)\n",
    "\n",
    "![Combined](https://i.imgur.com/5KuQcUS.png)\n",
    "\n",
    "This is the exact same method as linear regression. Only our cost function and the definition of our hypothesis has changed.\n",
    "\n",
    "A vectorized implementation is: \n",
    "\n",
    "&theta; = &theta; - &alpha; / m * X<sup>T</sup>(g(X&theta;) - y)\n",
    "\n",
    "If we want to predict a classification for a new term x, we output h(x) = 1 / (1 + e<sup>-&theta;<sup>T</sup>x</sup>), and interpret that as the probability that y = 1 for input x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Advanced optimization\n",
    "\n",
    "How to run logistic regression faster than gradient descent.\n",
    "\n",
    "More advanced algorithms:\n",
    "- Conjugate gradient\n",
    "- BFGS\n",
    "- L-BFGS\n",
    "\n",
    "These all require (1) a method for computing J(&theta;) and (2) a method for computing the partial derivative terms.\n",
    "\n",
    "What's special about these algorithms? They have a creative inner loop that picks a good learning rate &alpha; such that they converge faster than gradient descent.\n",
    "\n",
    "Advantages of these three algorithms:\n",
    "- No need to manually pick &alpha;\n",
    "- Often converge faster than gradient descent\n",
    "\n",
    "Disadvantages:\n",
    "- More complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Multiclass Classification\n",
    "\n",
    "Output y = {0, 1, 2, ...}\n",
    "\n",
    "![Multiclass](https://i.imgur.com/hJdAWYL.png)\n",
    "\n",
    "**One vs all classification:** Also called \"one vs rest.\" Turn the multiclass classification problem into multiple binary classification problems, and fit multiple classifiers onto the same graph (each classifier trained to recognize only one class). Then, when classifying input x, run all the classifiers and choose y that maximizes h<sub>&theta;</sub><sup>(i)</sup>(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
