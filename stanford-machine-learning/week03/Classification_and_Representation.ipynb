{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - Notes\n",
    "\n",
    "## Classification\n",
    "\n",
    "Examples: \n",
    "* Email: Spam or not spam?\n",
    "* Tumor: malignant or benign?\n",
    "* Online transactions: Fraudulent or not?\n",
    "\n",
    "Output y = {0, 1}, 0 = negative class (Malignant, etc), 1 = positive class (Benign, etc)\n",
    "\n",
    "These are **Binary classification problems.** Later we will address **Multiclass classification problems**.\n",
    "\n",
    "Applying linear regression to a classification problem is usually not a good idea. Instead, we use **logistic regression**, for which 0 <= h(x) <= 1.\n",
    "\n",
    "## Logistic regression model\n",
    "\n",
    "h<sub>&theta;</sub>(x) = g(&theta;<sup>T</sup>x), where g(z) = 1 / (1 + e<sup>-z</sup>)\n",
    "\n",
    "![Sigmoid function](https://i.imgur.com/Hmseltn.png)\n",
    "\n",
    "g(z) is called the **sigmoid function**, or the **logistic function**. The sigmoid function is chosen because (1) as z approaches negative infinity, g(z) approaches 0, and (2) as z approaches infinity, g(z) approaches 1.\n",
    "\n",
    "#### h<sub>&theta;</sub>(x) = 1 / (1 + e<sup>-&theta;<sup>T</sup>x</sup>) = _p(y = 1 | x ; &theta;)_ = _1 - p(y = 0 | x ; &theta;)_\n",
    "\n",
    "The output h(x) = the probability that y = 1 for input x (input is in the positive class) \n",
    "\n",
    "## Decision boundary\n",
    "\n",
    "Suppose, when h(x) >= 0.5, we predict y = 1, and when h(x) < 0.5 we predict y = 0. Where is the \"boundary\" for this decision? I.e. when does h(x) = 0.5? \n",
    "\n",
    "When you look at the graph of the sigmoid function, g(z) is >= 0.5 when z >= 0. Therefore, h(x) >= 0.5 (and therefore y = 1) when &theta;<sup>T</sup>x >= 0, and h(x) < 0.5 (and therefore y = 0) when &theta;<sup>T</sup>x < 0. \n",
    "\n",
    "Expand &theta;<sup>T</sup>x to get &theta;<sub>0</sub> + &theta;<sub>1</sub>x<sub>1</sub> + ... + &theta;<sub>n</sub>x<sub>n</sub> and set it equal to 0, and you get the formula for the line that separates the positive and negative classes on a graph, or the **decision boundary**.\n",
    "\n",
    "The training set is not used to find the decision boundary. Though the training set is used to coordinate the parameters, those parameters can then be used to find the decision boundary, without the help of training data. \n",
    "\n",
    "![Linear decision boundary](https://i.imgur.com/qKE7X1Q.png)\n",
    "\n",
    "Some decision boundaries are non-linear. To model a non-linear decision boundary, you can use a similar method to multivariate regression where you add higher order components to your hypothesis to fit a more complicated boundary.\n",
    "\n",
    "**Example:** h<sub>&theta;</sub> = g(&theta;<sub>0</sub> + &theta;<sub>1</sub>x<sub>1</sub> + &theta;<sub>2</sub>x<sub>2</sub><sup>2</sup> + &theta;<sub>3</sub>x<sub>3</sub><sup>2</sup>).\n",
    "\n",
    "**Summary:**\n",
    "* Predict y = 1 when &theta;<sup>T</sup>x >= 0\n",
    "* Predict y = 0 when &theta;<sup>T</sup>x < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
