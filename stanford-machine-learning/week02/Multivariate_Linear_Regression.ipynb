{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 - Notes\n",
    "\n",
    "---\n",
    "\n",
    "## Multivatiate linear regression\n",
    "\n",
    "**Notation:**\n",
    "* n = number of features\n",
    "* x<sup>(i)</sup> = input features of ith training example\n",
    "* x<sub>j</sub><sup>(i)</sup> = value of feature j in ith training example\n",
    "* For the convenience of notation, define x<sub>0</sub><sup>(i)</sup> = 1 (so x index starts at 0)\n",
    "\n",
    "**Example:**\n",
    "* x = \\[\\[2104, 5, 1, 45, 460\\], \\[1416, 3, 2, 40, 232\\], \\[1534, 3, 2, 30, 315\\], \\[852, 2, 1, 36, 178\\]\\]\n",
    "* n = 5\n",
    "* x<sup>(1)</sup> = \\[2104, 5, 1, 45, 460\\]\n",
    "* x<sub>2</sub><sup>(1)</sup> = 5\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "Previously (single feature): h<sub>&theta;</sub>(x) = &theta;<sub>0</sub> + &theta;<sub>1</sub>x\n",
    "\n",
    "Multiple features: h<sub>&theta;</sub>(x) = &theta;<sub>0</sub>x<sub>0</sub> + &theta;<sub>1</sub>x<sub>1</sub> + + &theta;<sub>2</sub>x<sub>2</sub> + ... + &theta;<sub>n</sub>x<sub>n</sub> \n",
    "\n",
    "For matrices, **_V_** is a vertical matrix and **_V<sup>T</sup>_** is a horizontal matrix.\n",
    "\n",
    "We can write &theta; as a matrix: &theta;<sup>T</sup> = \\[&theta;<sub>0</sub>, &theta;<sub>1</sub>, ... &theta;<sub>n</sub>\\]\n",
    "\n",
    "We can write x as a matrix: x<sup>T</sup> = \\[x<sub>0</sub>, x<sub>1</sub>, ... x<sub>n</sub>\\]\n",
    "\n",
    "Therefore, we can write the multivariate linear regression hypothesis h<sub>&theta;</sub>(x) as\n",
    "\n",
    "### = **&theta;<sup>T</sup>x**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent for multiple variables\n",
    "\n",
    "**Hypothesis:** h<sub>&theta;</sub>(x) = &theta;<sup>T</sup>x\n",
    "\n",
    "**Features:** &theta;\n",
    "\n",
    "**Cost Function:** J(&theta;) = 1 / 2m &Sigma;( h<sub>&theta;</sub>(x<sub>(i)</sub>) - y<sub>(i)</sub> )<sup>2</sup>\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "![gradient descent](https://i.imgur.com/pMKnb4f.png)\n",
    "\n",
    "Then, after plugging our multivaraite hypothesis function into the gradient descent algorithm:\n",
    "\n",
    "![gradient descent hypothesis](https://i.imgur.com/X4Gcj5Z.png)\n",
    "\n",
    "\n",
    "This new algorithm is essentially the same as before (the single feature case). \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Techniques to make gradient descent perform better with multiple variables in practice:\n",
    "\n",
    "## Feature scaling\n",
    "\n",
    "Idea: Make sure, in a problem with multiple features, that those features are on a similar scale. Feature scaling can speed up convergence during gradient descent.\n",
    "\n",
    "Eg: \n",
    "\n",
    "**Before scaling:** x<sub>1</sub> = size (0 - 2000 ft<sup>2</sup>), x<sub>2</sub> = number of bedrooms (1-5)\n",
    "\n",
    "**After scaling:** x<sub>1</sub> = size ft<sup>2</sup> / 2000, x<sub>2</sub> = number of bedrooms / 5\n",
    "\n",
    "So that (approximately) 0 <= x<sub>j</sub> <=1 for all j in |x|\n",
    "\n",
    "## Mean normalization\n",
    "\n",
    "Replace x<sub>i</sub> with x<sub>i</sub> - &mu;<sub>i</sub> to make features have approximately zero mean (does not apply to x<sub>0</sub> = 1)\n",
    "\n",
    "Or, alternatively, replace x<sub>i</sub> with (x<sub>i</sub> - &mu;<sub>i</sub>) / s<sub>i</sub>, where s<sub>i</sub> is either the range of x<sup>(i)</sup> (max - min) or the standard deviation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we make sure gradient descent is working correctly?**\n",
    "\n",
    "![plot the cost function](https://i.imgur.com/s6zGUY1.png)\n",
    "\n",
    "1. **Plot** the cost function J(&theta;) after every iteration of gradient descent, and ensure that it is decreasing with every iteration. Looking at this figure can also help you determine whether J(&theta) has converged.\n",
    "2. Establish an \"Automatic convergence test\"; A sample convergence test: declare convergence if J(&theta;) decreases by less than some value (e.g. 10<sup>-3</sup>) after iteration.\n",
    "3. For a sufficiently small a, J(&theta;) **should** decrease on every iteration. If J(&theta;) diverges or refuses to stabilize (looking at the graph can help you detect this), you may need to decrease the learning rate a.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Python implementation of the multivariate regression\n",
    "# using batch gradient descent\n",
    "# Input: training set (x, y)\n",
    "#        a, learning rate\n",
    "#        N, convergence criteria\n",
    "# Output: [theta_0, theta_1, ... theta_n], parameters of J such that\n",
    "#         J([theta_0, theta_1, ... theta_n]) is at a global minimum\n",
    "def multivariate_regression(x, y, a, N):\n",
    "    m = len(x)\n",
    "    n = 0\n",
    "    if m > -1:\n",
    "        n = x[0].shape[1]\n",
    "    theta = [0 for _ in range(n)]\n",
    "    \n",
    "    while True:\n",
    "        theta_sum = [0 for _ in range(n)]\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                A = np.matrix(theta)\n",
    "                B = np.matrix(x[i]).transpose()\n",
    "                theta_sum[j] += (np.asscalar(np.matmul(A, B)) - y[i]) * x[i,j]\n",
    "        convergence = 1\n",
    "        for j in range(n):\n",
    "            convergence = convergence and (abs(theta_sum[j]) < N)\n",
    "        if convergence:\n",
    "            break\n",
    "        else:\n",
    "            for j in range(n):\n",
    "                theta[j] = theta[j] - (a / m) * theta_sum[j]\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.192758903904197, 0.539655929622876]\n"
     ]
    }
   ],
   "source": [
    "# Testing multivariate regression\n",
    "\n",
    "a = np.matrix([\n",
    "  [3.9,8.94],\n",
    "  [5.4,10.85],\n",
    "  [5.8,11.61],\n",
    "  [6,13.65],\n",
    "  [6.5,13.54],\n",
    "  [6.1,13.29],\n",
    "  [5.9,17.65],\n",
    "  [5.5,18.81],\n",
    "  [5.4,17.91]\n",
    "  ])\n",
    "b = [8.73,14.28,17.68,9.94,14.99,18.75,11.4,15.08,19.3]\n",
    "\n",
    "print(multivariate_regression(a,b,.001,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression\n",
    "\n",
    "If we look at the distribution of our data and decide that a linear regression is not appropriate, we can try to fit a polynomial regression to it.\n",
    "\n",
    "**Linear regression hypothesis:** h<sub>&theta;</sub>(x) = &theta;<sub>0</sub>x<sub>0</sub> + &theta;<sub>1</sub>x<sub>1</sub> + + &theta;<sub>2</sub>x<sub>2</sub> + ... + &theta;<sub>n</sub>x<sub>n</sub>\n",
    "\n",
    "**Polynomial regression hypothesis:** h<sub>&theta;</sub>(x) = &theta;<sub>0</sub>x<sub>0</sub><sup>0</sup> + &theta;<sub>1</sub>x<sub>1</sub><sup>1</sup> + + &theta;<sub>2</sub>x<sub>2</sub><sup>2</sup> + ... + &theta;<sub>n</sub>x<sub>n</sub><sup>n</sup>\n",
    "\n",
    "With polynomial regression, feature scaling is **_extremely_** important.\n",
    "\n",
    "You also have some choice in what feature to use for your model. E.g. rather than just using the frontage and the depth of a house plot, you can use the area (frontage * depth), or the area and the square root of the area, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
