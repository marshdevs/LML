{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - Notes\n",
    "\n",
    "---\n",
    "\n",
    "# Model and Cost Function\n",
    "\n",
    "## Global symbols:\n",
    "\n",
    "* m = size of training set\n",
    "* x = input variables / features\n",
    "* y = output variable / target\n",
    "* (x, y) = one member of the training set\n",
    "* (x<sub>(i)</sub>, y<sub>(i)</sub>) = ith training example\n",
    "* h = \"hypothesis\"; a function that maps x -> y\n",
    "\n",
    "Our goal for the supervised learning problem, given a training set x,y, is to write a function h: X -> Y such that h(x) is a good predictor for all x and their corresponding values of y. When the target variable (y) has continuous values, this is called _regression_. When it's discrete, we call it _classification_.\n",
    "\n",
    "![Learning algorithm](https://i.imgur.com/cdkilZ7.png)\n",
    "\n",
    "**How do we represent h?**\n",
    "\n",
    "_Hypothesis_ = h<sub>&theta;</sub>(x) = &theta;<sub>o</sub> x + &theta;<sub>1</sub>x\n",
    "_(shorthand: h(x))_\n",
    "\n",
    "**Example:**\n",
    "\n",
    "**Learning Algorithm:** Size of house (x) --> \\[ h \\] --> Estimated price (y)\n",
    "\n",
    "### Univariate linear regression of one variable\n",
    "\n",
    "![Univariate linear regression of one variable](https://i.imgur.com/ONXY9GV.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Cost function\n",
    "\n",
    "_Hypothesis_ = h<sub>&theta;</sub>(x) = &theta;<sub>o</sub> x + &theta;<sub>1</sub>x\n",
    "\n",
    "&theta;<sub>i</sub>'s = parameters\n",
    "\n",
    "**_The supervised regression problem:_** Come up with values for all parameters (in linear regression, values for &theta;<sub>o</sub> and &theta;<sub>1</sub>) so that the hypothesis function fits the training set.\n",
    "\n",
    "**_Solution_**: Minimize over &theta;<sub>o</sub> and &theta;<sub>1</sub> the difference between h(x) and the right answer for all members of the training set. \n",
    "\n",
    "In plain English, we want to minimize a _Cost function_. Our cost function computes, over the training set, the average squared difference between the prediction of the hypothesis for the i<sup>th</sup> element of the input and the actual i<sup>th</sup> element of the output.\n",
    "\n",
    "Formally, the cost function is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J(&theta;<sub>o</sub>, &theta;<sub>1</sub>) = 1 / 2m &Sigma;( h<sub>&theta;</sub>(x<sub>(i)</sub>) - y<sub>(i)</sub> )<sup>2</sup>\n",
    "\n",
    "min<sub>&theta;<sub>o</sub>,&theta;<sub>1</sub></sub> J(&theta;<sub>o</sub>, &theta;<sub>1</sub>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called the **Squared error cost function.**\n",
    "\n",
    "---\n",
    "\n",
    "### Q:\n",
    "\n",
    "> The sum of the squared errors is divided by m to compute the average squared error. But why is it also divided by 2?\n",
    "\n",
    "The mean is halved as a convenience for the computation of the **_gradient descent_**, as the derivative term of the square function will cancel out the 1/2 term. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the cost function\n",
    "\n",
    "### J(&theta;<sub>1</sub>)\n",
    "\n",
    "![Cost function 1 feature](https://i.imgur.com/wKHKd32.png)\n",
    "\n",
    "Above is a depiction of the cost function J(&theta;<sub>1</sub>) and a single hypothesis h(x) for a single feature. As J(&theta;<sub>1</sub>) approaches 1, h(x) starts to model the training set better, as the value that minimizes the cost function is J(&theta;<sub>1</sub>) = 1.\n",
    "\n",
    "![Cost function 2 features](https://i.imgur.com/HA5xrro.png)\n",
    "\n",
    "This is a three-dimensional depiction of a cost function J(&theta;<sub>0</sub>, &theta;<sub>1</sub>) over two features. A slightly more readable version of this graph is called a **contour plot**.\n",
    "\n",
    "![Contour plot cost function 2 features](https://i.imgur.com/WHcaFg6.png)\n",
    "\n",
    "This contour plot represents a cost function J(&theta;<sub>0</sub>, &theta;<sub>1</sub>) over two features. Beside it is shown h(x) for a given pair (&theta;<sub>0</sub>, &theta;<sub>1</sub>). Lines on the contour plot are colored to represent the height of the cost function for the given pair (&theta;<sub>0</sub>, &theta;<sub>1</sub>). Notice at the center of the nested elipses, is the darkest point, the point at which the cost function is minimized.\n",
    "\n",
    "---\n",
    "\n",
    "Now we want an efficient algorithm to compute parameters &theta;<sub>i</sub>'s that minimize the cost function J."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
